{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8椎体骨折検出モデル学習ノートブック\n",
    "\n",
    "このノートブックは`train_yolo.py`スクリプトを対話的に実行するためのものです。\n",
    "\n",
    "## 概要\n",
    "- YOLOv8を使用した椎体骨折検出モデルの学習\n",
    "- 医療画像に特化した設定\n",
    "- Weights & Biasesによる実験管理\n",
    "- モデルの評価とエクスポート機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境設定と依存関係のインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "プロジェクトルート: /mnt/nfs1/home/yamamoto-hiroto/research/vertebrae_saka\n",
      "CUDA利用可能: True\n",
      "CUDA デバイス: NVIDIA RTX A6000\n",
      "CUDA メモリ: 47.5 GB\n"
     ]
    }
   ],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "# プロジェクトルートをパスに追加\n",
    "project_root = Path(os.getcwd()).parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# train_yolo.pyからクラスをインポート\n",
    "from train_yolo import VertebralFractureYOLO\n",
    "\n",
    "print(f\"プロジェクトルート: {project_root}\")\n",
    "print(f\"CUDA利用可能: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA デバイス: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA メモリ: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 設定ファイルの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "設定ファイルパス: /mnt/nfs1/home/yamamoto-hiroto/research/vertebrae_saka/Sakaguchi_file/YOLO_datasets/vertebrae_fracture/configs/vertebrae_fracture.yaml\n",
      "設定ファイル存在: True\n",
      "\n",
      "設定ファイル内容:\n",
      "  path: /mnt/nfs1/home/yamamoto-hiroto/research/vertebrae_saka/Sakaguchi_file/YOLO_datasets/vertebrae_fracture\n",
      "  train: train/images\n",
      "  val: val/images\n",
      "  test: test/images\n",
      "  nc: 1\n",
      "  names: ['fracture']\n"
     ]
    }
   ],
   "source": [
    "# データセット設定ファイルのパス\n",
    "config_path = project_root / \"Sakaguchi_file/YOLO_datasets/vertebrae_fracture/configs/vertebrae_fracture.yaml\"\n",
    "\n",
    "print(f\"設定ファイルパス: {config_path}\")\n",
    "print(f\"設定ファイル存在: {config_path.exists()}\")\n",
    "\n",
    "# 設定ファイルの内容を表示\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"\\n設定ファイル内容:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"⚠️ 設定ファイルが見つかりません。データセットの準備を確認してください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. YOLOトレーナーの初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 18:28:16,517 - INFO - 設定ファイル読み込み完了: /mnt/nfs1/home/yamamoto-hiroto/research/vertebrae_saka/Sakaguchi_file/YOLO_datasets/vertebrae_fracture/configs/vertebrae_fracture.yaml\n",
      "2025-07-17 18:28:16,621 - INFO - YOLOv8モデル初期化: yolov8m.pt\n",
      "2025-07-17 18:28:16,622 - INFO - 出力ディレクトリ: runs/train/20250717_182816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルサイズ: yolov8m.pt\n",
      "エポック数: 50\n",
      "バッチサイズ: 16\n",
      "Weights & Biases使用: True\n",
      "\n",
      "✅ YOLOトレーナーの初期化完了\n"
     ]
    }
   ],
   "source": [
    "# 学習パラメータの設定\n",
    "MODEL_SIZE = 'yolov8m.pt'  # yolov8n.pt, yolov8s.pt, yolov8m.pt, yolov8l.pt, yolov8x.pt\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 16\n",
    "USE_WANDB = True  # Weights & Biasesを使用するかどうか\n",
    "\n",
    "print(f\"モデルサイズ: {MODEL_SIZE}\")\n",
    "print(f\"エポック数: {EPOCHS}\")\n",
    "print(f\"バッチサイズ: {BATCH_SIZE}\")\n",
    "print(f\"Weights & Biases使用: {USE_WANDB}\")\n",
    "\n",
    "# トレーナーを初期化\n",
    "trainer = VertebralFractureYOLO(\n",
    "    config_path=str(config_path),\n",
    "    model_size=MODEL_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\n✅ YOLOトレーナーの初期化完了\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Weights & Biasesセットアップ（オプション）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# wandBでの学習曲線表示設定（S_model_learning準拠）\nprint(\"📊 学習曲線はwandBダッシュボードで確認します\")\nprint(\"💡 以下の手順で学習曲線をリアルタイムで監視できます：\")\nprint()\nprint(\"1. 🌐 https://wandb.ai にアクセス\")\nprint(\"2. 📊 プロジェクト 'vertebrae-fracture-axial-1' を選択\")\nprint(\"3. 📈 実行中の実験をクリック\")\nprint(\"4. 📊 'Charts' タブで学習曲線を確認\")\nprint()\nprint(\"📈 監視すべき主要メトリクス:\")\nprint(\"   • train/box_loss - 学習用Box Loss\")\nprint(\"   • val/box_loss - 検証用Box Loss\")\nprint(\"   • train/cls_loss - 学習用分類Loss\")\nprint(\"   • val/cls_loss - 検証用分類Loss\")\nprint(\"   • metrics/mAP50(B) - mAP@0.5\")\nprint(\"   • metrics/mAP50-95(B) - mAP@0.5:0.95\")\nprint(\"   • metrics/precision(B) - 精度\")\nprint(\"   • metrics/recall(B) - 再現率\")\nprint()\nprint(\"💡 学習開始後、wandBダッシュボードで学習曲線がリアルタイムで更新されます\")\nprint(\"✅ wandB学習曲線監視システム準備完了\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 学習パラメータの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習パラメータ:\n",
      "  data: /mnt/nfs1/home/yamamoto-hiroto/research/vertebrae_saka/Sakaguchi_file/YOLO_datasets/vertebrae_fracture/configs/vertebrae_fracture.yaml\n",
      "  epochs: 50\n",
      "  batch: 16\n",
      "  imgsz: 640\n",
      "  device: cuda\n",
      "  lr0: 0.01\n",
      "  lrf: 0.01\n",
      "  momentum: 0.937\n",
      "  weight_decay: 0.0005\n",
      "  hsv_h: 0.015\n",
      "  hsv_s: 0.3\n",
      "  hsv_v: 0.3\n",
      "  degrees: 20\n",
      "  translate: 0.1\n",
      "  scale: 0.2\n",
      "  shear: 0.1\n",
      "  perspective: 0.0\n",
      "  flipud: 0.0\n",
      "  fliplr: 0.5\n",
      "  mosaic: 0.8\n",
      "  mixup: 0.1\n",
      "  copy_paste: 0.1\n",
      "  dropout: 0.0\n",
      "  label_smoothing: 0.0\n",
      "  val: True\n",
      "  save_period: 10\n",
      "  save_json: True\n",
      "  project: runs/train\n",
      "  name: 20250717_182816\n",
      "  patience: 30\n",
      "  workers: 4\n",
      "  seed: 42\n",
      "  deterministic: True\n",
      "  single_cls: True\n",
      "  rect: True\n",
      "  cos_lr: True\n",
      "  close_mosaic: 10\n",
      "  resume: False\n",
      "  amp: True\n",
      "  fraction: 1.0\n",
      "  profile: False\n",
      "  freeze: None\n",
      "  multi_scale: True\n",
      "  overlap_mask: True\n",
      "  mask_ratio: 4\n",
      "  box: 7.5\n",
      "  cls: 0.5\n",
      "  dfl: 1.5\n",
      "  pose: 12.0\n",
      "  kobj: 2.0\n",
      "  nbs: 64\n",
      "  optimizer: auto\n"
     ]
    }
   ],
   "source": [
    "# 学習パラメータを取得して表示\n",
    "params = trainer.get_training_params()\n",
    "params['epochs'] = EPOCHS\n",
    "params['batch'] = BATCH_SIZE\n",
    "\n",
    "print(\"学習パラメータ:\")\n",
    "for key, value in params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. モデル学習の実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 最終的な学習曲線の表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# wandBでの学習結果確認\nprint(\"📊 学習結果の確認\")\nprint(\"=\" * 50)\n\n# 学習完了の確認\nif 'results' in locals():\n    print(\"✅ 学習が正常に完了しました\")\n    \n    # 学習サマリーの保存\n    trainer.save_training_summary(results)\n    print(\"📄 学習サマリーを保存しました\")\n    \n    # wandBでの確認を促す\n    print(\"\\\\n📊 詳細な学習曲線と結果の確認:\")\n    print(\"🌐 https://wandb.ai にアクセス\")\n    print(\"📊 プロジェクト: vertebrae-fracture-axial-1\")\n    print(\"📈 以下の学習曲線を確認してください:\")\n    print(\"   • Loss curves (train/val)\")\n    print(\"   • mAP curves\")\n    print(\"   • Precision/Recall curves\")\n    print(\"   • Learning rate schedule\")\n    print(\"   • Model predictions\")\n    \n    # 最終メトリクスの簡易表示\n    if hasattr(results, 'results_dict'):\n        print(\"\\\\n📈 最終メトリクス（概要）:\")\n        metrics = results.results_dict\n        for key, value in metrics.items():\n            if 'mAP' in key or 'precision' in key or 'recall' in key:\n                print(f\"   {key}: {value:.4f}\")\n    \n    # 保存されたモデルの確認\n    best_model_path = trainer.output_dir / \"weights\" / \"best.pt\"\n    last_model_path = trainer.output_dir / \"weights\" / \"last.pt\"\n    \n    print(f\"\\\\n💾 保存されたモデル:\")\n    if best_model_path.exists():\n        print(f\"   🏆 Best model: {best_model_path}\")\n    if last_model_path.exists():\n        print(f\"   📝 Last model: {last_model_path}\")\n        \n    # YOLOv8が自動生成した結果画像の確認\n    results_png = trainer.output_dir / \"results.png\"\n    if results_png.exists():\n        print(f\"\\\\n📊 YOLOv8自動生成学習曲線: {results_png}\")\n        print(\"   (画像ファイルを直接開いて確認してください)\")\n    \n    print(\"\\\\n💡 学習曲線の詳細分析はwandBダッシュボードで行ってください\")\n    \nelse:\n    print(\"⚠️ 学習結果が利用できません\")\n    print(\"学習を完了してから再実行してください\")"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 18:13:56,995 - INFO - YOLOv8モデル学習開始\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習開始時刻: 2025-07-17 18:13:56\n",
      "==================================================\n",
      "WARNING ⚠️ 'label_smoothing' is deprecated and will be removed in in the future.\n",
      "Ultralytics 8.3.167 🚀 Python-3.12.8 torch-2.4.0+cu121 CUDA:0 (NVIDIA RTX A6000, 48677MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.1, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/mnt/nfs1/home/yamamoto-hiroto/research/vertebrae_saka/Sakaguchi_file/YOLO_datasets/vertebrae_fracture/configs/vertebrae_fracture.yaml, degrees=20, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.3, hsv_v=0.3, imgsz=640, int8=False, iou=0.7, keras=False, kobj=2.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.1, mode=train, model=yolov8m.pt, momentum=0.937, mosaic=0.8, multi_scale=True, name=20250717_1813002, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=30, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=True, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/train/20250717_1813002, save_frames=False, save_json=True, save_period=10, save_txt=False, scale=0.2, seed=42, shear=0.1, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding class names with single class.\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3776275  ultralytics.nn.modules.head.Detect           [1, [192, 384, 576]]          \n",
      "Model summary: 169 layers, 25,856,899 parameters, 25,856,883 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.35M/5.35M [00:00<00:00, 86.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.2±0.0 ms, read: 86.3±25.7 MB/s, size: 27.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/nfs1/home/yamamoto-hiroto/research/vertebrae_saka/Sakaguchi_file/YOLO_datasets/vertebrae_fracture/train/labels.cache... 37367 images, 33979 backgrounds, 0 corrupt: 100%|██████████| 37367/37367 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "WARNING ⚠️ 'rect=True' is incompatible with DataLoader shuffle, setting shuffle=False\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.2±0.0 ms, read: 88.8±8.4 MB/s, size: 31.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/nfs1/home/yamamoto-hiroto/research/vertebrae_saka/Sakaguchi_file/YOLO_datasets/vertebrae_fracture/val/labels.cache... 10843 images, 10357 backgrounds, 0 corrupt: 100%|██████████| 10843/10843 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/train/20250717_1813002/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/20250717_1813002\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      13.9G     0.4534      8.413     0.4053          0        544:  38%|███▊      | 892/2336 [02:42<04:23,  5.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 学習実行\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     results = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# 学習完了\u001b[39;00m\n\u001b[32m     11\u001b[39m     end_time = datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/vertebrae_saka/Sakaguchi_file/YOLO_training/train_yolo.py:174\u001b[39m, in \u001b[36mVertebralFractureYOLO.train\u001b[39m\u001b[34m(self, epochs, batch_size)\u001b[39m\n\u001b[32m    170\u001b[39m params[\u001b[33m'\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m'\u001b[39m] = batch_size\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# 学習実行\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33m学習完了\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/vertebrae_saka/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:799\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    796\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    798\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/vertebrae_saka/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:227\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    224\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/vertebrae_saka/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:419\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m     last_opt_step = ni\n\u001b[32m    422\u001b[39m     \u001b[38;5;66;03m# Timed stopping\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/vertebrae_saka/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:646\u001b[39m, in \u001b[36mBaseTrainer.optimizer_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    644\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ema:\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/research/vertebrae_saka/.venv/lib/python3.12/site-packages/ultralytics/utils/torch_utils.py:692\u001b[39m, in \u001b[36mModelEMA.update\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    690\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m v.dtype.is_floating_point:  \u001b[38;5;66;03m# true for FP16 and FP32\u001b[39;00m\n\u001b[32m    691\u001b[39m     v *= d\n\u001b[32m--> \u001b[39m\u001b[32m692\u001b[39m     v += (\u001b[32m1\u001b[39m - d) * msd[k].detach()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 学習開始時刻を記録\n",
    "start_time = datetime.now()\n",
    "print(f\"学習開始時刻: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 学習実行\n",
    "try:\n",
    "    results = trainer.train(epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # 学習完了\n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"✅ 学習完了!\")\n",
    "    print(f\"学習時間: {duration}\")\n",
    "    print(f\"学習完了時刻: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 学習エラー: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 学習結果の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習結果の表示\n",
    "if 'results' in locals():\n",
    "    print(\"学習結果:\")\n",
    "    if hasattr(results, 'results_dict'):\n",
    "        for key, value in results.results_dict.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # 学習サマリーの保存\n",
    "    trainer.save_training_summary(results)\n",
    "    print(\"\\n✅ 学習サマリーを保存しました\")\n",
    "else:\n",
    "    print(\"⚠️ 学習結果が利用できません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. モデルの評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検証データセットでの評価\n",
    "try:\n",
    "    print(\"検証データセットでの評価を実行中...\")\n",
    "    val_results = trainer.evaluate(data_split='val')\n",
    "    \n",
    "    print(\"\\n✅ 評価完了\")\n",
    "    print(f\"評価結果: {val_results}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 評価エラー: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. サンプル推論（オプション）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプル画像での推論テスト\n",
    "# 推論テスト用の画像パスを指定\n",
    "sample_image_path = None  # 実際のテスト画像パスを指定\n",
    "\n",
    "if sample_image_path and Path(sample_image_path).exists():\n",
    "    try:\n",
    "        print(f\"サンプル推論: {sample_image_path}\")\n",
    "        inference_results = trainer.predict_sample(sample_image_path, conf_threshold=0.25)\n",
    "        print(f\"推論結果: {inference_results}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 推論エラー: {e}\")\n",
    "else:\n",
    "    print(\"⏭️ サンプル推論をスキップ（画像パスが指定されていません）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. モデルエクスポート（オプション）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルをONNX形式でエクスポート\n",
    "export_format = 'onnx'  # 'onnx', 'torchscript', 'tensorrt'\n",
    "\n",
    "try:\n",
    "    print(f\"モデルを{export_format}形式でエクスポート中...\")\n",
    "    exported_model = trainer.export_model(format=export_format)\n",
    "    print(f\"✅ エクスポート完了: {exported_model}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ エクスポートエラー: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 後処理とクリーンアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights & Biasesセッションの終了\n",
    "if USE_WANDB:\n",
    "    try:\n",
    "        import wandb\n",
    "        wandb.finish()\n",
    "        print(\"✅ Weights & Biases セッション終了\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Weights & Biases セッション終了エラー: {e}\")\n",
    "\n",
    "# 出力ディレクトリの表示\n",
    "print(f\"\\n📁 出力ディレクトリ: {trainer.output_dir}\")\n",
    "print(f\"📁 学習結果の保存場所: {trainer.output_dir}\")\n",
    "\n",
    "# 完了メッセージ\n",
    "print(\"\\n🎉 全ての処理が完了しました！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vertebrae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}